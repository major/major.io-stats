name: Publish blog stats

on:
  push:
    branches:
      - "main"
  schedule:
    - cron: '45 11 * * *'
  workflow_dispatch:

permissions:
  id-token: write
  contents: read
  pages: write

defaults:
  run:
    shell: bash

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Set time zone
        uses: szenius/set-timezone@1f9716b0f7120e344f0c62bb7b1ee98819aefd42 # v2.0
        with:
          timezoneLinux: "America/Chicago"
    
      - name: Checkout
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5

      - name: Restore logs from cache
        uses: actions/cache/restore@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        with:
          path: logs
          key: logs-cache-${{ github.run_id }}
          restore-keys: |
            logs-cache-

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@a03048d87541d1d9fcf2ecf528a4a65ba9bd7838 # v5.0.0
        with:
          role-to-assume: arn:aws:iam::911986281031:role/github-actions-major.io-stats
          role-duration-seconds: 900
          aws-region: us-east-1

      - name: Create directories
        run: mkdir -p logs public

      - name: Download log files
        run: |
          #!/bin/bash
          # Create a marker file to track the last sync time
          MARKER_FILE="logs/.last-sync"

          # If we have previously downloaded logs, only sync files newer than the marker
          if [ -f "$MARKER_FILE" ]; then
            echo "Incremental sync: downloading only new files since last run"
            LAST_SYNC=$(cat "$MARKER_FILE")
            # Use --exclude with --include to filter by modification time isn't directly supported,
            # so we'll list files first and download only newer ones
            aws s3 ls s3://major.io-logs/ --recursive | \
              awk -v last="$LAST_SYNC" '$1" "$2 > last {print $4}' | \
              while read -r file; do
                aws s3 cp "s3://major.io-logs/$file" "logs/$file"
              done
          else
            echo "Initial sync: downloading all log files"
            aws s3 sync --size-only s3://major.io-logs/ logs/
          fi

          # Update the marker file with current timestamp
          date -u '+%Y-%m-%d %H:%M:%S' > "$MARKER_FILE"

      - name: Prepare geoip databases
        run: |
          pushd geoip
          tar --zstd -xf geoip.tar.zst
          popd

      - name: Run goaccess
        run: |
          find logs -name "*.gz" | \
            xargs zcat | \
            docker run --rm -i -v ./geoip:/geoip:ro -e LANG=$LANG docker.io/allinurl/goaccess \
              --log-format CLOUDFRONT \
              --date-format CLOUDFRONT \
              --time-format CLOUDFRONT \
              --agent-list \
              --ignore-status=301 \
              --ignore-status=302 \
              --ignore-crawlers \
              --real-os \
              --anonymize-ip \
              --http-protocol=yes \
              --http-method=yes \
              --no-query-string \
              --geoip-database /geoip/GeoLite2-City.mmdb \
              --geoip-database /geoip/GeoLite2-Country.mmdb \
              --geoip-database /geoip/GeoLite2-ASN.mmdb \
              --output html - > public/index.html

      - name: Setup Pages
        id: pages
        uses: actions/configure-pages@983d7736d9b0ae728b81ab479565c72886d7745b # v5

      - name: Save logs to cache
        uses: actions/cache/save@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        with:
          path: logs
          key: logs-cache-${{ github.run_id }}

      - name: Upload artifact
        uses: actions/upload-pages-artifact@7b1f4a764d45c48632c6b24a0339c27f5614fb0b # v4
        with:
          path: ./public

  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@d6db90164ac5ed86f2b6aed7e0febac5b3c0c03e # v4
